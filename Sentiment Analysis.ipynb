{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "512b64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import tokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dc4a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "nltk.download('stopwords')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc812b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Twitter_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ddfb6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ca48f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING TEXTTT\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;].')\n",
    "REPLACE_BY_NULL = re.compile('[#]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    #text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_NULL.sub('', text)# lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b4cbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda,GRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f717fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text     object\n",
       "category      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e7c5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['clean_text'].astype(str).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f2d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = tokenizer.texts_to_sequences(df['clean_text'].astype(str).values)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90459f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 160\n",
    "textdata = pad_sequences(df['clean_text'], maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "deca56a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  42,    1,  307,   66, 1726, 1119,   40, 2378,    2, 1211,  205,\n",
       "          2,  215,   32,  155,  100,   49,   69, 1068,  215,   50,    3,\n",
       "          6,  546,    3,   50, 4179, 5582,    3, 2806,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90115abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fdbbdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(textdata,df.drop(columns=['clean_text']), test_size = 0.25, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1a1bf70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train['category'],dummy_na=False)\n",
    "y_test = pd.get_dummies(y_test['category'],dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9ac0ac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122235, 3)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f8f368ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, SpatialDropout1D, LSTM, GRU,Conv1D\n",
    "#from tensorflow.keras.layers import add,concatenate,Conv1DMaxPooling1D,\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ec256e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x1 (InputLayer)                [(None, 160)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_23 (Embedding)       (None, 160, 100)     11368000    ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 160, 128)    63744       ['embedding_23[0][0]']           \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 158, 64)      24640       ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 64)          0           ['conv1d_2[0][0]']               \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 64)          0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 128)          0           ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 3)            387         ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,456,771\n",
      "Trainable params: 88,771\n",
      "Non-trainable params: 11,368,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_layer1 = Input(shape=(textdata.shape[1],) ,name='x1')\n",
    "input_layer2 = Input(shape=(1,), name='x2')\n",
    "embed_layer = Embedding(vocab_size, embedding_matrix.shape[1],   weights=[embedding_matrix], input_length=maxlen , trainable=False)(input_layer1)\n",
    "x = Bidirectional(GRU(64, return_sequences=True, dropout=0.5,recurrent_dropout=0.5))(embed_layer)\n",
    "x = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = concatenate([ avg_pool, max_pool])\n",
    "\n",
    "preds = Dense((3), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_layer1, preds)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', \n",
    "              #metrics=['binary_crossentropy']\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e75ef9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3056/3056 [==============================] - 561s 182ms/step - loss: 0.7381 - acc: 0.6768 - val_loss: 0.5787 - val_acc: 0.7689\n",
      "Epoch 2/5\n",
      "3056/3056 [==============================] - 557s 182ms/step - loss: 0.6014 - acc: 0.7544 - val_loss: 0.4645 - val_acc: 0.8284\n",
      "Epoch 3/5\n",
      "3056/3056 [==============================] - 560s 183ms/step - loss: 0.5244 - acc: 0.7951 - val_loss: 0.3990 - val_acc: 0.8612\n",
      "Epoch 4/5\n",
      "3056/3056 [==============================] - 559s 183ms/step - loss: 0.4756 - acc: 0.8218 - val_loss: 0.3698 - val_acc: 0.8789\n",
      "Epoch 5/5\n",
      "3056/3056 [==============================] - 561s 183ms/step - loss: 0.4463 - acc: 0.8366 - val_loss: 0.3520 - val_acc: 0.8873\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, batch_size=32, epochs=5, verbose=1, validation_split=0.2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3339d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NLP_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "45311f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8803534175972512"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred = np.round(pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
